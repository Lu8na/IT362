{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIkjxAWWygfJ42ThkihDza",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lu8na/IT362/blob/main/Logbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQr2euTjIxl3",
        "outputId": "ca01d8ce-b962-477c-b20a-1a0ab886dceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Places collected: 0\n",
            "Total reviews collected: 0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json, time, random\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "# SETTINGS - Configuration variables for the script\n",
        "API_KEY = \"AIzaSyBFu74b6afMqfo5t8koIMwgenEIDBfVVtc\"  # Google Places API key\n",
        "\n",
        "# API endpoints for Google Places\n",
        "NEARBY_URL  = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
        "DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "\n",
        "# Collection targets and parameters\n",
        "TARGET_PLACES = 700  # Target number of places to collect\n",
        "RADIUS = 4000  # Search radius in meters\n",
        "PLACE_TYPES = [\"beauty_salon\", \"spa\"]  # Types of businesses to search for\n",
        "\n",
        "# List of UK city coordinates (latitude, longitude)\n",
        "UK_CITIES = [\n",
        "    (51.5074, -0.1278),   # London\n",
        "    (53.4808, -2.2426),   # Manchester\n",
        "    (52.4862, -1.8904),   # Birmingham\n",
        "    (55.8642, -4.2518),   # Glasgow\n",
        "    (53.8008, -1.5491),   # Leeds\n",
        "    (51.4545, -2.5879),   # Bristol\n",
        "    (50.8225, -0.1372),   # Brighton\n",
        "    (52.9548, -1.1581),   # Nottingham\n",
        "    (51.4816, -3.1791),   # Cardiff\n",
        "    (54.9783, -1.6178)    # Newcastle\n",
        "]\n",
        "\n",
        "# OUTPUT FILES - File paths for storing data\n",
        "RAW_SEARCH_FILE   = \"raw_search_responses.jsonl\"  # Raw API responses\n",
        "UNSTRUCTURED_FILE = \"unstructured_reviews.jsonl\"  # Extracted reviews\n",
        "\n",
        "# Helper function to make API calls with retry logic\n",
        "def get_json(url, params):\n",
        "    for attempt in range(5):  # Try up to 5 times\n",
        "        r = requests.get(url, params=params, timeout=30)\n",
        "        data = r.json()\n",
        "        if data.get(\"status\") == \"OVER_QUERY_LIMIT\":  # If rate limited\n",
        "            time.sleep(2**attempt + random.random())  # Exponential backoff\n",
        "            continue\n",
        "        return data\n",
        "    return data\n",
        "\n",
        "\n",
        "# MAIN EXECUTION STARTS HERE\n",
        "\n",
        "seen = set()  # Track seen place IDs to avoid duplicates\n",
        "place_ids = []  # Store collected place IDs\n",
        "\n",
        "# PHASE 1: COLLECT PLACE IDs FROM NEARBY SEARCH\n",
        "with open(RAW_SEARCH_FILE, \"w\", encoding=\"utf-8\") as raw_f:\n",
        "    # Iterate through all UK cities\n",
        "    for lat, lng in UK_CITIES:\n",
        "        # Iterate through each place type (beauty_salon, spa)\n",
        "        for t in PLACE_TYPES:\n",
        "\n",
        "            # Make API call to find places nearby\n",
        "            data = get_json(NEARBY_URL, {\n",
        "                \"location\": f\"{lat},{lng}\",\n",
        "                \"radius\": RADIUS,\n",
        "                \"type\": t,\n",
        "                \"key\": API_KEY\n",
        "            })\n",
        "\n",
        "            # Save raw response to file for debugging/backup\n",
        "            raw_f.write(json.dumps({\n",
        "                \"type\": t,\n",
        "                \"lat\": lat,\n",
        "                \"lng\": lng,\n",
        "                \"response\": data,\n",
        "                \"collected_at\": datetime.now().isoformat()\n",
        "            }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "            # Skip if API returned an error\n",
        "            if data.get(\"status\") not in (\"OK\", \"ZERO_RESULTS\"):\n",
        "                continue\n",
        "\n",
        "            # Process paginated results (up to 3 pages)\n",
        "            page = 1\n",
        "            while True:\n",
        "                # Extract place IDs from results\n",
        "                for p in data.get(\"results\", []):\n",
        "                    pid = p.get(\"place_id\")\n",
        "                    if pid and pid not in seen:  # Avoid duplicates\n",
        "                        seen.add(pid)\n",
        "                        place_ids.append(pid)\n",
        "                        if len(place_ids) >= TARGET_PLACES:\n",
        "                            break\n",
        "\n",
        "                # Stop if we reached target number of places\n",
        "                if len(place_ids) >= TARGET_PLACES:\n",
        "                    break\n",
        "\n",
        "                # Check for next page of results\n",
        "                token = data.get(\"next_page_token\")\n",
        "                if not token or page >= 3:  # Limit to 3 pages\n",
        "                    break\n",
        "\n",
        "                # Wait before requesting next page (API requirement)\n",
        "                time.sleep(2)\n",
        "                data = get_json(NEARBY_URL, {\n",
        "                    \"pagetoken\": token,\n",
        "                    \"key\": API_KEY\n",
        "                })\n",
        "                page += 1\n",
        "\n",
        "            # Break loops if target reached\n",
        "            if len(place_ids) >= TARGET_PLACES:\n",
        "                break\n",
        "        if len(place_ids) >= TARGET_PLACES:\n",
        "            break\n",
        "\n",
        "print(\"Places collected:\", len(place_ids))\n",
        "\n",
        "# PHASE 2: FETCH REVIEWS FOR EACH PLACE\n",
        "reviews_count = 0\n",
        "\n",
        "with open(UNSTRUCTURED_FILE, \"w\", encoding=\"utf-8\") as out_f:\n",
        "    # For each collected place ID, get detailed information including reviews\n",
        "    for pid in place_ids:\n",
        "\n",
        "        # Request place details (only reviews field to save quota)\n",
        "        d = get_json(DETAILS_URL, {\n",
        "            \"place_id\": pid,\n",
        "            \"fields\": \"reviews\",\n",
        "            \"key\": API_KEY\n",
        "        })\n",
        "\n",
        "        # Process reviews if API call successful\n",
        "        if d.get(\"status\") == \"OK\":\n",
        "            revs = d.get(\"result\", {}).get(\"reviews\", []) or []\n",
        "            for rev in revs:\n",
        "                txt = rev.get(\"text\")\n",
        "                if not txt:  # Skip empty reviews\n",
        "                    continue\n",
        "\n",
        "                # Save each review with metadata\n",
        "                out_f.write(json.dumps({\n",
        "                    \"place_id\": pid,\n",
        "                    \"review_text\": txt,\n",
        "                    \"rating\": rev.get(\"rating\"),\n",
        "                    \"time\": rev.get(\"time\"),\n",
        "                    \"relative_time_description\": rev.get(\"relative_time_description\"),\n",
        "                    \"language\": rev.get(\"language\")\n",
        "                }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "                reviews_count += 1\n",
        "\n",
        "        # Rate limiting delay between requests\n",
        "        time.sleep(0.2)\n",
        "\n",
        "print(\"Total reviews collected:\", reviews_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for JSON handling and data manipulation\n",
        "import json  # For parsing JSON data from the file\n",
        "import pandas as pd  # For creating and manipulating DataFrames\n",
        "\n",
        "# Initialize an empty list to store each review as a dictionary/row\n",
        "rows = []\n",
        "\n",
        "# Open the JSONL file (JSON Lines format - each line is a separate JSON object)\n",
        "with open(\"unstructured_reviews.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    # Read the file line by line\n",
        "    for line in f:\n",
        "        # Parse the JSON string from the current line into a Python dictionary\n",
        "        obj = json.loads(line)\n",
        "\n",
        "        # Extract specific fields from the JSON object and structure them into a clean dictionary\n",
        "        rows.append({\n",
        "            \"place_id\": obj.get(\"place_id\"),  # Unique identifier for the place/business\n",
        "            \"review_text\": obj.get(\"review_text\"),  # The actual review content written by the user\n",
        "            \"rating\": obj.get(\"rating\"),  # Star rating (usually 1-5)\n",
        "            \"time\": obj.get(\"time\"),  # Timestamp of when the review was written\n",
        "            \"relative_time_description\": obj.get(\"relative_time_description\"),  # Human-readable time (e.g., \"2 weeks ago\")\n",
        "            \"language\": obj.get(\"language\")  # Language of the review (if detected by Google)\n",
        "        })\n",
        "\n",
        "# Convert the list of dictionaries into a pandas DataFrame\n",
        "# Each dictionary becomes a row, keys become column headers\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Export the DataFrame to a CSV file for easy viewing and analysis\n",
        "df.to_csv(\n",
        "    \"unstructured_reviews_readable.csv\",  # Output filename\n",
        "    index=False,  # Don't include the automatic row index in the CSV\n",
        "    encoding=\"utf-8-sig\"  # UTF-8 with BOM for better Excel compatibility\n",
        ")"
      ],
      "metadata": {
        "id": "SAOMcc3YJSGo"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}